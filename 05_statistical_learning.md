# üéì **Lecture 5: Statistical Learning**
*"Teach Computers to Guess Like a Pro (Without the Crystal Ball)!"*

---

## üß† **What‚Äôs the Big Idea?**
Computers can **learn from data**‚Äîjust like how you learn to predict if it‚Äôll rain by looking at clouds. This lecture is all about:
1. **Statistics**: The math behind making **smart guesses** (like "Will my team win the game?").
2. **Machine Learning**: Teaching computers to **spot patterns** (like sorting M&Ms by color or recognizing cats in photos).
3. **Real-World Magic**: Using data to **predict the future** (kinda).

**Why‚Äôs this important?**
- **Sports**: Predict who‚Äôll win the next match.
- **Weather**: Guess if it‚Äôll rain tomorrow.
- **Games**: Teach a computer to play **Rock-Paper-Scissors** (and cheat at it).
- **Security**: Spot weird stuff (like a hacker trying to break in).

---

## üöÇ **Analogy Time!**
### **Statistical Learning = Detective Work üïµÔ∏è‚Äç‚ôÇÔ∏è**
Imagine you‚Äôre a **detective** trying to solve a mystery:
- **Clues (Data)**: Footprints, fingerprints, and witness statements.
- **Suspects (Predictions)**: Who stole the cookie? Was it the butler? The cat?
- **Solution (Model)**: You **connect the dots** and make a **smart guess** about who did it.

### **Machine Learning = Training a Dog üêï**
Imagine you‚Äôre teaching your dog to **fetch**:
- **Show it a ball** (data).
- **Say "fetch"** (command).
- **Reward it with a treat** (positive feedback).
- **Repeat** until the dog **learns** to fetch on its own.

**Machine learning is the same!** You show the computer **examples**, give it **feedback**, and it **learns** to make predictions.

---

## üîç **Key Concepts (The Magic Behind the Scenes)**
### **Statistics (The Math of Guessing)**
Statistics helps us **make sense of data**. It‚Äôs like **counting candies** to figure out which flavor is the most popular.

**Key Ideas:**
- **Mean (Average)**: The **middle** of a bunch of numbers.
  - Example: If you have `[3, 5, 7]`, the mean is `(3 + 5 + 7) / 3 = 5`.
- **Median**: The **middle number** when you sort them.
  - Example: In `[3, 5, 7]`, the median is `5`.
- **Mode**: The **most common** number.
  - Example: In `[3, 5, 5, 7]`, the mode is `5`.
- **Standard Deviation**: How **spread out** the numbers are.
  - Example: `[1, 5, 9]` is **more spread out** than `[4, 5, 6]`.

**Fun Fact:**
If you flip a coin 100 times, you‚Äôll get **about 50 heads and 50 tails**‚Äîbut not exactly. Statistics helps us **predict** that!

---

### **Machine Learning (Teaching Computers to Learn)**
Machine learning is about **teaching computers to spot patterns**‚Äîlike how you recognize your friend‚Äôs face or your favorite cereal box.

**Types of Machine Learning:**
| Type | Example | Real-World Use |
|------|---------|----------------|
| **Supervised Learning** | Teaching a computer to recognize cats vs. dogs. | Spam filters, face detection |
| **Unsupervised Learning** | Sorting M&Ms by color without knowing the colors. | Customer segmentation, anomaly detection |
| **Reinforcement Learning** | Teaching a computer to play a game by rewarding wins. | Self-driving cars, robotics |

**Fun Fact:**
Netflix uses **machine learning** to **guess** which shows you‚Äôll like!

---

### **Training a Model (The "Learning" Part)**
Training a model is like **teaching a kid to ride a bike**:
1. **Show them examples** (data).
2. **Let them try** (make predictions).
3. **Give feedback** (correct mistakes).
4. **Repeat** until they **get it right**.

**Example:**
Teaching a computer to **predict house prices**:
- **Data**: Size of the house, number of bedrooms, location.
- **Model**: A **math formula** that guesses the price.
- **Training**: Show the computer **real house prices** and let it **learn** the pattern.

---

## üöÄ **Let‚Äôs Get Hands-On!**
### **Predict the Weather (Statistics)**
Let‚Äôs use **statistics** to predict if it‚Äôll rain tomorrow based on **cloud cover**!

```python
import numpy as np
import matplotlib.pyplot as plt

# Data: Cloud cover (%) and whether it rained (1 = yes, 0 = no)
cloud_cover = np.array([10, 20, 30, 40, 50, 60, 70, 80, 90, 100])
rain = np.array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1])

# Calculate the mean cloud cover when it rained vs. didn't rain
mean_rain = np.mean(cloud_cover[rain == 1])
mean_no_rain = np.mean(cloud_cover[rain == 0])

print(f"Mean cloud cover when it rained: {mean_rain}%")
print(f"Mean cloud cover when it didn't rain: {mean_no_rain}%")

# Plot the data
plt.scatter(cloud_cover, rain, color="blue")
plt.axvline(mean_rain, color="red", linestyle="--", label="Mean (Rain)")
plt.axvline(mean_no_rain, color="green", linestyle="--", label="Mean (No Rain)")
plt.xlabel("Cloud Cover (%)")
plt.ylabel("Rain (1 = Yes, 0 = No)")
plt.title("Cloud Cover vs. Rain")
plt.legend()
plt.show()
```

**What‚Äôs happening?**
1. We **collect data** (cloud cover vs. rain).
2. We **calculate the mean** cloud cover for rainy vs. non-rainy days.
3. We **plot the data** to see the pattern.

**Challenge:**
Try **adding more data** (like temperature or humidity) to make the prediction **more accurate**!

---

### **Sort M&Ms by Color (Unsupervised Learning)**
Let‚Äôs use **K-Means Clustering** (a fancy way of saying "sort into groups") to **sort M&Ms by color**!

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Generate random M&M colors (RGB values)
np.random.seed(42)
colors = np.random.rand(100, 3)  # 100 M&Ms, 3 colors (R, G, B)

# Use K-Means to sort them into 3 groups (red, green, blue)
kmeans = KMeans(n_clusters=3)
kmeans.fit(colors)
labels = kmeans.labels_

# Plot the M&Ms
plt.scatter(colors[:, 0], colors[:, 1], c=labels, cmap="viridis")
plt.title("M&Ms Sorted by Color")
plt.xlabel("Red")
plt.ylabel("Green")
plt.show()
```

**What‚Äôs happening?**
1. We **generate random M&M colors** (RGB values).
2. We **use K-Means** to sort them into **3 groups** (red, green, blue).
3. We **plot the M&Ms** to see how they‚Äôre sorted.

**Challenge:**
Try **changing the number of groups** (like 4 or 5) to see how the sorting changes!

---

### **Teach a Computer to Play Rock-Paper-Scissors (Reinforcement Learning)**
Let‚Äôs use **reinforcement learning** to teach a computer to **play Rock-Paper-Scissors**‚Äîand **cheat at it**!

```python
import random

# Define the choices
choices = ["rock", "paper", "scissors"]

# Initialize the computer's strategy (start with random)
computer_strategy = [1, 1, 1]  # Equal probability for each choice

# Play 10 rounds
for round in range(10):
    # Get the user's choice
    user_choice = input("Choose rock, paper, or scissors: ").lower()

    # Computer chooses based on its strategy
    computer_choice = random.choices(choices, weights=computer_strategy)[0]

    # Determine the winner
    if user_choice == computer_choice:
        print(f"Tie! Both chose {user_choice}.")
    elif (
        (user_choice == "rock" and computer_choice == "scissors") or
        (user_choice == "paper" and computer_choice == "rock") or
        (user_choice == "scissors" and computer_choice == "paper")
    ):
        print(f"You win! {user_choice} beats {computer_choice}.")
        # Update strategy: Decrease the probability of the computer's choice
        computer_strategy[choices.index(computer_choice)] -= 0.1
    else:
        print(f"You lose! {computer_choice} beats {user_choice}.")
        # Update strategy: Increase the probability of the computer's choice
        computer_strategy[choices.index(computer_choice)] += 0.1

    # Normalize the strategy (so probabilities add up to 1)
    computer_strategy = [max(0, x) for x in computer_strategy]
    total = sum(computer_strategy)
    computer_strategy = [x / total for x in computer_strategy]

    print(f"Computer's strategy: {dict(zip(choices, computer_strategy))}")
```

**What‚Äôs happening?**
1. The computer **starts with a random strategy** (equal chance for rock, paper, scissors).
2. After each round, the computer **updates its strategy** based on whether it won or lost.
3. Over time, the computer **learns to cheat** by picking the choice that **beats you most often**!

**Challenge:**
Try **playing more rounds** to see if the computer **gets better at cheating**!

---

### **4Ô∏è‚É£ Predict House Prices (Supervised Learning)**
Let‚Äôs use **linear regression** (a fancy way of saying "draw a straight line through data") to **predict house prices**!

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Data: House size (sq ft) vs. price ($)
house_sizes = np.array([1000, 1500, 2000, 2500, 3000, 3500, 4000]).reshape(-1, 1)
house_prices = np.array([300000, 400000, 500000, 600000, 700000, 800000, 900000])

# Train a linear regression model
model = LinearRegression()
model.fit(house_sizes, house_prices)

# Predict the price of a 2750 sq ft house
predicted_price = model.predict([[2750]])
print(f"Predicted price for a 2750 sq ft house: ${predicted_price[0]:,.2f}")

# Plot the data and the regression line
plt.scatter(house_sizes, house_prices, color="blue", label="Actual Prices")
plt.plot(house_sizes, model.predict(house_sizes), color="red", label="Predicted Prices")
plt.xlabel("House Size (sq ft)")
plt.ylabel("Price ($)")
plt.title("House Size vs. Price")
plt.legend()
plt.show()
```

**What‚Äôs happening?**
1. We **collect data** (house sizes vs. prices).
2. We **train a linear regression model** to find the pattern.
3. We **predict the price** of a new house.
4. We **plot the data** to see how well the model fits.

**Challenge:**
Try **adding more data** (like number of bedrooms or location) to make the prediction **more accurate**!

---

## üéØ **Challenge Time!**
### **Challenge 1: Predict Your Grades**
Use **linear regression** to predict your **final grade** based on your **homework scores**!

**Hint:**
- **Data**: Homework scores vs. final grades.
- **Model**: Train a linear regression model.
- **Prediction**: Guess your final grade based on your latest homework score.

---

### **Challenge 2: Sort Your Toy Collection**
Use **K-Means Clustering** to sort your **toy collection** by **color, size, or type**!

**Hint:**
- **Data**: RGB values for each toy‚Äôs color.
- **Model**: Use K-Means to sort them into groups.
- **Visualization**: Plot the toys to see how they‚Äôre sorted.

---

### **Challenge 3: Teach a Computer to Play Tic-Tac-Toe**
Use **reinforcement learning** to teach a computer to **play Tic-Tac-Toe**‚Äîand **never lose**!

**Hint:**
- **Data**: Past games (who won, who lost).
- **Model**: Update the computer‚Äôs strategy based on wins/losses.
- **Game**: Play against the computer and see if it **gets better over time**!

---

## üìö **Summary**
In this lecture, you learned:
1. **Statistics**: The math behind making **smart guesses** (mean, median, mode).
2. **Machine Learning**: Teaching computers to **spot patterns** (supervised, unsupervised, reinforcement).
3. **How to apply this in real life**: Predicting the weather, sorting M&Ms, playing games, and guessing house prices.

---

## üöÄ **What‚Äôs Next?**
In **Lecture 6**, we‚Äôll dive into **Cognitive Processing**‚Äîteaching computers to **"think" like humans** (like recognizing emotions or understanding language).

**Ready to level up?** Let‚Äôs go! üöÄ